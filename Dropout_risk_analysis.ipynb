{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtPgAwGXhvGYNXlwc3ilw2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DzangaR67/Data-science-project_Dropout-analysis/blob/main/Dropout_risk_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0yQM3-e7Akr"
      },
      "outputs": [],
      "source": [
        "# requirements.txt\n",
        "\"\"\"\n",
        "pandas==2.0.3\n",
        "numpy==1.24.3\n",
        "scikit-learn==1.3.0\n",
        "matplotlib==3.7.2\n",
        "seaborn==0.12.2\n",
        "plotly==5.15.0\n",
        "streamlit==1.28.0\n",
        "xgboost==1.7.6\n",
        "imbalanced-learn==0.10.1\n",
        "jupyter==1.0.0\n",
        "joblib==1.3.0\n",
        "ucimlrepo==0.0.6\n",
        "\"\"\"\n",
        "# Install all required packages\n",
        "!pip install ucimlrepo pandas numpy scikit-learn matplotlib seaborn plotly xgboost imbalanced-learn joblib\n",
        "\n",
        "print(\" All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Student Dropout Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data manipulation\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                           f1_score, roc_auc_score, confusion_matrix,\n",
        "                           classification_report, precision_recall_curve,\n",
        "                           roc_curve)\n",
        "\n",
        "# Statistical tests\n",
        "from scipy.stats import chi2_contingency, ttest_ind\n",
        "\n",
        "# Handle imbalanced data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Fetch dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "print(\"Starting Student Dropout Analysis with Real Dataset...\")\n",
        "\n",
        "# Fetch dataset\n",
        "print(\"Fetching dataset from UCI repository...\")\n",
        "predict_students_dropout_and_academic_success = fetch_ucirepo(id=697)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = predict_students_dropout_and_academic_success.data.features\n",
        "y = predict_students_dropout_and_academic_success.data.targets\n",
        "\n",
        "# Metadata and variable information\n",
        "print(\"Dataset Information:\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Target variable: {y.columns[0]}\")\n",
        "print(f\"Target distribution:\\n{y.iloc[:, 0].value_counts()}\")\n",
        "\n",
        "# Create a copy for manipulation\n",
        "df = X.copy()\n",
        "df['target'] = y.iloc[:, 0]\n",
        "\n",
        "print(\"\\n Target Variable Analysis:\")\n",
        "print(df['target'].value_counts())\n",
        "print(f\"Dataset size: {len(df)} students\")\n",
        "\n",
        "# Convert target to binary: Dropout vs Not Dropout\n",
        "df['dropout_risk'] = (df['target'] == 'Dropout').astype(int)\n",
        "print(f\"\\nDropout rate: {df['dropout_risk'].mean():.2%}\")\n",
        "\n",
        "# Explore the dataset\n",
        "print(\"\\n Dataset Overview:\")\n",
        "print(df.info())\n",
        "print(\"\\n Basic Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Data Preprocessing Class\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def explore_missing_values(self):\n",
        "        \"\"\"Explore and handle missing values\"\"\"\n",
        "        missing_values = self.df.isnull().sum()\n",
        "        print(\"Missing values per column:\")\n",
        "        print(missing_values[missing_values > 0])\n",
        "\n",
        "        if missing_values.sum() == 0:\n",
        "            print(\"No missing values found!\")\n",
        "        return self.df\n",
        "\n",
        "    def encode_categorical_variables(self):\n",
        "        \"\"\"Encode categorical variables\"\"\"\n",
        "        categorical_columns = self.df.select_dtypes(include=['object']).columns\n",
        "\n",
        "        for col in categorical_columns:\n",
        "            if col != 'target':  # Don't encode the original target\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                self.df[col] = self.label_encoders[col].fit_transform(self.df[col].astype(str))\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def feature_engineering(self):\n",
        "        \"\"\"Create new features based on domain knowledge\"\"\"\n",
        "        # Create academic performance score\n",
        "        grade_columns = [col for col in self.df.columns if 'grade' in col.lower() or 'grade' in col]\n",
        "        if grade_columns:\n",
        "            self.df['avg_grade'] = self.df[grade_columns].mean(axis=1)\n",
        "\n",
        "        # Create attendance consistency feature\n",
        "        approved_columns = [col for col in self.df.columns if 'approved' in col.lower()]\n",
        "        if len(approved_columns) >= 2:\n",
        "            self.df['approval_consistency'] = (self.df[approved_columns[0]] - self.df[approved_columns[1]]).abs()\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def prepare_features(self):\n",
        "        \"\"\"Prepare final feature set\"\"\"\n",
        "        # Drop original target and any other non-feature columns\n",
        "        features_to_drop = ['target', 'dropout_risk']\n",
        "\n",
        "        # Select features for modeling\n",
        "        feature_columns = [col for col in self.df.columns if col not in features_to_drop]\n",
        "\n",
        "        X = self.df[feature_columns]\n",
        "        y = self.df['dropout_risk']\n",
        "\n",
        "        return X, y, feature_columns\n",
        "\n",
        "    def preprocess_pipeline(self):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        print(\"Step 1: Exploring missing values...\")\n",
        "        self.explore_missing_values()\n",
        "\n",
        "        print(\"Step 2: Encoding categorical variables...\")\n",
        "        self.encode_categorical_variables()\n",
        "\n",
        "        print(\"Step 3: Feature engineering...\")\n",
        "        self.feature_engineering()\n",
        "\n",
        "        print(\"Step 4: Preparing final features...\")\n",
        "        X, y, feature_columns = self.prepare_features()\n",
        "\n",
        "        print(f\"Final dataset shape: {X.shape}\")\n",
        "        print(f\"Target distribution: {y.value_counts()}\")\n",
        "        print(f\"Dropout risk rate: {y.mean():.2%}\")\n",
        "\n",
        "        return X, y, feature_columns\n",
        "\n",
        "# Execute preprocessing\n",
        "print(\"\\nüîß Starting Data Preprocessing...\")\n",
        "preprocessor = DataPreprocessor(df)\n",
        "X, y, feature_columns = preprocessor.preprocess_pipeline()\n",
        "processed_df = preprocessor.df\n",
        "\n",
        "print(f\"\\n Preprocessing completed!\")\n",
        "print(f\"Features: {len(feature_columns)}\")\n",
        "print(f\"Sample feature names: {feature_columns[:10]}...\")\n"
      ],
      "metadata": {
        "id": "ZDueMhX-72Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis\n",
        "print(\"\\n Starting Comprehensive Exploratory Data Analysis...\")\n",
        "\n",
        "class StudentDataAnalyzer:\n",
        "    def __init__(self, df, target_col='dropout_risk'):\n",
        "        self.df = df\n",
        "        self.target_col = target_col\n",
        "\n",
        "    def plot_target_distribution(self):\n",
        "        \"\"\"Plot distribution of target variable\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        risk_counts = self.df[self.target_col].value_counts()\n",
        "        colors = ['#2ecc71', '#e74c3c']\n",
        "\n",
        "        # Count plot\n",
        "        ax1.bar(['Not At-Risk', 'At-Risk'], risk_counts.values, color=colors, alpha=0.7)\n",
        "        ax1.set_title('Distribution of Dropout Risk', fontsize=14, fontweight='bold')\n",
        "        ax1.set_ylabel('Number of Students')\n",
        "\n",
        "        # Add percentage labels\n",
        "        total = len(self.df)\n",
        "        for i, count in enumerate(risk_counts.values):\n",
        "            ax1.text(i, count + 5, f'{count}\\n({count/total:.1%})',\n",
        "                    ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        # Pie chart\n",
        "        ax2.pie(risk_counts.values, labels=['Not At-Risk', 'At-Risk'],\n",
        "               autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "        ax2.set_title('Dropout Risk Proportion')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Dropout Risk Rate: {risk_counts[1]/total:.2%}\")\n",
        "        return risk_counts\n",
        "\n",
        "    def plot_feature_distributions(self):\n",
        "        \"\"\"Plot distributions of key features\"\"\"\n",
        "        # Select key numerical features for visualization\n",
        "        numerical_features = self.df.select_dtypes(include=[np.number]).columns\n",
        "        # Remove target from features\n",
        "        numerical_features = [f for f in numerical_features if f != self.target_col]\n",
        "\n",
        "        # Take first 4 numerical features\n",
        "        key_features = numerical_features[:4]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i, feature in enumerate(key_features):\n",
        "            if i < len(axes):\n",
        "                # Distribution by risk status\n",
        "                for risk_status in [0, 1]:\n",
        "                    data = self.df[self.df[self.target_col] == risk_status][feature]\n",
        "                    axes[i].hist(data, alpha=0.6, label=f'Risk: {risk_status}',\n",
        "                               bins=15, density=True)\n",
        "\n",
        "                axes[i].set_title(f'Distribution of {feature}', fontweight='bold')\n",
        "                axes[i].set_xlabel(feature)\n",
        "                axes[i].set_ylabel('Density')\n",
        "                axes[i].legend()\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_categorical_relationships(self):\n",
        "        \"\"\"Plot relationships between categorical features and dropout risk\"\"\"\n",
        "        # Find categorical features (encoded)\n",
        "        categorical_features = []\n",
        "        for col in self.df.columns:\n",
        "            if self.df[col].dtype == 'object' and col != 'target':\n",
        "                if self.df[col].nunique() < 10:  # Only plot features with few categories\n",
        "                    categorical_features.append(col)\n",
        "\n",
        "        if not categorical_features:\n",
        "            print(\"No suitable categorical features found for visualization.\")\n",
        "            return\n",
        "\n",
        "        # Take first 4 categorical features\n",
        "        categorical_features = categorical_features[:4]\n",
        "\n",
        "        n_plots = len(categorical_features)\n",
        "        n_cols = 2\n",
        "        n_rows = (n_plots + 1) // n_cols\n",
        "\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "        if n_plots == 1:\n",
        "            axes = [axes]\n",
        "        else:\n",
        "            axes = axes.ravel()\n",
        "\n",
        "        for i, feature in enumerate(categorical_features):\n",
        "            if i < len(axes):\n",
        "                # Create contingency table\n",
        "                contingency_table = pd.crosstab(self.df[feature], self.df[self.target_col])\n",
        "                contingency_table_perc = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
        "\n",
        "                # Plot\n",
        "                contingency_table_perc.plot(kind='bar', ax=axes[i],\n",
        "                                          color=['#2ecc71', '#e74c3c'], alpha=0.7)\n",
        "                axes[i].set_title(f'Dropout Risk by {feature}', fontweight='bold')\n",
        "                axes[i].set_xlabel(feature)\n",
        "                axes[i].set_ylabel('Proportion')\n",
        "                axes[i].legend(['Not At-Risk', 'At-Risk'])\n",
        "                axes[i].tick_params(axis='x', rotation=45)\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Remove empty subplots\n",
        "        for i in range(n_plots, len(axes)):\n",
        "            fig.delaxes(axes[i])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def correlation_analysis(self):\n",
        "        \"\"\"Plot correlation heatmap\"\"\"\n",
        "        # Select numerical features for correlation\n",
        "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
        "        # Limit to top 15 features to avoid overcrowding\n",
        "        if len(numerical_cols) > 15:\n",
        "            numerical_cols = numerical_cols[:15]\n",
        "\n",
        "        corr_matrix = self.df[numerical_cols].corr()\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "                   center=0, fmt='.2f', linewidths=0.5)\n",
        "        plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Show top correlations with target\n",
        "        if self.target_col in corr_matrix.columns:\n",
        "            target_correlations = corr_matrix[self.target_col].abs().sort_values(ascending=False)\n",
        "            print(\"Top features correlated with dropout risk:\")\n",
        "            print(target_correlations.head(10))\n",
        "\n",
        "            return target_correlations\n",
        "        return None\n",
        "\n",
        "    def statistical_tests(self):\n",
        "        \"\"\"Perform statistical tests\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"STATISTICAL ANALYSIS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Select numerical features for t-test\n",
        "        numerical_features = self.df.select_dtypes(include=[np.number]).columns\n",
        "        # Remove target from features\n",
        "        numerical_features = [f for f in numerical_features if f != self.target_col]\n",
        "\n",
        "        # Take first 5 numerical features\n",
        "        test_features = numerical_features[:5]\n",
        "\n",
        "        print(\"\\nT-Test Results (At-Risk vs Not At-Risk):\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for feature in test_features:\n",
        "            group1 = self.df[self.df[self.target_col] == 0][feature]  # Not at-risk\n",
        "            group2 = self.df[self.df[self.target_col] == 1][feature]  # At-risk\n",
        "\n",
        "            if len(group1) > 1 and len(group2) > 1:  # Ensure we have data\n",
        "                t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
        "\n",
        "                significance = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''\n",
        "                print(f\"{feature:30}: t-stat = {t_stat:7.3f}, p-value = {p_value:.4f} {significance}\")\n",
        "\n",
        "    def comprehensive_analysis(self):\n",
        "        \"\"\"Run all analysis methods\"\"\"\n",
        "        print(\"STARTING COMPREHENSIVE EXPLORATORY DATA ANALYSIS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. Target distribution\n",
        "        print(\"\\n1. TARGET VARIABLE DISTRIBUTION\")\n",
        "        self.plot_target_distribution()\n",
        "\n",
        "        # 2. Feature distributions\n",
        "        print(\"\\n2. FEATURE DISTRIBUTIONS\")\n",
        "        self.plot_feature_distributions()\n",
        "\n",
        "        # 3. Categorical relationships\n",
        "        print(\"\\n3. CATEGORICAL FEATURES ANALYSIS\")\n",
        "        self.plot_categorical_relationships()\n",
        "\n",
        "        # 4. Correlation analysis\n",
        "        print(\"\\n4. CORRELATION ANALYSIS\")\n",
        "        target_correlations = self.correlation_analysis()\n",
        "\n",
        "        # 5. Statistical tests\n",
        "        self.statistical_tests()\n",
        "\n",
        "        return target_correlations\n",
        "\n",
        "# Execute EDA\n",
        "analyzer = StudentDataAnalyzer(processed_df)\n",
        "target_correlations = analyzer.comprehensive_analysis()"
      ],
      "metadata": {
        "id": "WCU1w2GK7lNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training and Evaluation\n",
        "print(\"\\n Starting Model Training and Evaluation...\")\n",
        "\n",
        "class DropoutPredictor:\n",
        "    def __init__(self, X, y, feature_names):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.feature_names = feature_names\n",
        "        self.models = {}\n",
        "        self.predictions = {}\n",
        "        self.metrics = {}\n",
        "        self.best_model = None\n",
        "\n",
        "        # Split data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {self.X_train.shape[0]} samples\")\n",
        "        print(f\"Test set: {self.X_test.shape[0]} samples\")\n",
        "        print(f\"Dropout rate in training: {self.y_train.mean():.2%}\")\n",
        "        print(f\"Dropout rate in test: {self.y_test.mean():.2%}\")\n",
        "\n",
        "        # Handle class imbalance\n",
        "        self.handle_imbalance()\n",
        "\n",
        "        # Scale features\n",
        "        self.scale_features()\n",
        "\n",
        "    def handle_imbalance(self):\n",
        "        \"\"\"Handle class imbalance using SMOTE\"\"\"\n",
        "        print(\"\\nOriginal class distribution:\")\n",
        "        print(self.y_train.value_counts())\n",
        "\n",
        "        smote = SMOTE(random_state=42)\n",
        "        self.X_train_resampled, self.y_train_resampled = smote.fit_resample(self.X_train, self.y_train)\n",
        "\n",
        "        print(\"After SMOTE resampling:\")\n",
        "        print(pd.Series(self.y_train_resampled).value_counts())\n",
        "\n",
        "    def scale_features(self):\n",
        "        \"\"\"Scale numerical features\"\"\"\n",
        "        self.scaler = StandardScaler()\n",
        "        self.X_train_scaled = self.scaler.fit_transform(self.X_train_resampled)\n",
        "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
        "\n",
        "    def train_models(self):\n",
        "        \"\"\"Train multiple models\"\"\"\n",
        "        print(\"\\n Training Machine Learning Models...\")\n",
        "\n",
        "        # Define models with basic parameters for faster training\n",
        "        models = {\n",
        "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "        }\n",
        "\n",
        "        for name, model in models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "            model.fit(self.X_train_scaled, self.y_train_resampled)\n",
        "            self.models[name] = model\n",
        "\n",
        "            # Make predictions\n",
        "            self.predictions[name] = model.predict(self.X_test_scaled)\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MODEL EVALUATION RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        evaluation_results = []\n",
        "\n",
        "        for name in self.models.keys():\n",
        "            y_pred = self.predictions[name]\n",
        "            y_pred_proba = self.models[name].predict_proba(self.X_test_scaled)[:, 1]\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = {\n",
        "                'Model': name,\n",
        "                'Accuracy': accuracy_score(self.y_test, y_pred),\n",
        "                'Precision': precision_score(self.y_test, y_pred),\n",
        "                'Recall': recall_score(self.y_test, y_pred),\n",
        "                'F1-Score': f1_score(self.y_test, y_pred),\n",
        "                'ROC-AUC': roc_auc_score(self.y_test, y_pred_proba)\n",
        "            }\n",
        "\n",
        "            self.metrics[name] = metrics\n",
        "            evaluation_results.append(metrics)\n",
        "\n",
        "            # Print confusion matrix\n",
        "            print(f\"\\n{name} - Confusion Matrix:\")\n",
        "            cm = confusion_matrix(self.y_test, y_pred)\n",
        "            self.plot_confusion_matrix(cm, name)\n",
        "\n",
        "        # Create results dataframe\n",
        "        results_df = pd.DataFrame(evaluation_results)\n",
        "        results_df = results_df.round(4)\n",
        "\n",
        "        print(\"\\nComparative Model Performance:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(results_df.to_string(index=False))\n",
        "\n",
        "        # Identify best model based on F1-score\n",
        "        self.best_model_name = results_df.loc[results_df['F1-Score'].idxmax(), 'Model']\n",
        "        self.best_model = self.models[self.best_model_name]\n",
        "\n",
        "        print(f\"\\n BEST MODEL: {self.best_model_name}\")\n",
        "        print(f\"Best F1-Score: {results_df['F1-Score'].max():.4f}\")\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def plot_confusion_matrix(self, cm, model_name):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Not At-Risk', 'At-Risk'],\n",
        "                   yticklabels=['Not At-Risk', 'At-Risk'])\n",
        "        plt.title(f'Confusion Matrix - {model_name}', fontweight='bold')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.show()\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"Classification Report:\")\n",
        "        y_pred = self.predictions[model_name]\n",
        "        print(classification_report(self.y_test, y_pred,\n",
        "                                  target_names=['Not At-Risk', 'At-Risk']))\n",
        "\n",
        "    def plot_roc_curves(self):\n",
        "        \"\"\"Plot ROC curves for all models\"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1]\n",
        "            fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
        "            roc_auc = roc_auc_score(self.y_test, y_pred_proba)\n",
        "\n",
        "            plt.plot(fpr, tpr, lw=2,\n",
        "                    label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curves - Model Comparison', fontweight='bold')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    def feature_importance_analysis(self):\n",
        "        \"\"\"Analyze and plot feature importance\"\"\"\n",
        "        if hasattr(self.best_model, 'feature_importances_'):\n",
        "            importances = self.best_model.feature_importances_\n",
        "            feature_importance_df = pd.DataFrame({\n",
        "                'feature': self.feature_names,\n",
        "                'importance': importances\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.barplot(data=feature_importance_df.head(15),\n",
        "                       x='importance', y='feature', palette='viridis')\n",
        "            plt.title(f'Top 15 Feature Importances - {self.best_model_name}',\n",
        "                     fontweight='bold')\n",
        "            plt.xlabel('Importance Score')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(\"Top 10 Most Important Features:\")\n",
        "            print(feature_importance_df.head(10).round(4).to_string(index=False))\n",
        "\n",
        "            return feature_importance_df\n",
        "        else:\n",
        "            print(\"Feature importance not available for this model type.\")\n",
        "            return None\n",
        "\n",
        "    def cross_validation_scores(self):\n",
        "        \"\"\"Perform cross-validation\"\"\"\n",
        "        print(\"\\n Cross-Validation Scores (F1):\")\n",
        "        cv_scores = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            scores = cross_val_score(model, self.X_train_scaled, self.y_train_resampled,\n",
        "                                   cv=5, scoring='f1', n_jobs=-1)\n",
        "            cv_scores[name] = scores\n",
        "            print(f\"{name:20}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
        "\n",
        "        return cv_scores\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the trained model and preprocessor\"\"\"\n",
        "        import joblib\n",
        "\n",
        "        model_artifacts = {\n",
        "            'best_model': self.best_model,\n",
        "            'scaler': self.scaler,\n",
        "            'feature_names': self.feature_names,\n",
        "            'metrics': self.metrics\n",
        "        }\n",
        "\n",
        "        joblib.dump(model_artifacts, 'student_dropout_model.pkl')\n",
        "        print(\"Model saved as 'student_dropout_model.pkl'\")\n",
        "\n",
        "    def full_pipeline(self):\n",
        "        \"\"\"Execute complete modeling pipeline\"\"\"\n",
        "        print(\"STARTING MODEL TRAINING PIPELINE\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # 1. Train models\n",
        "        self.train_models()\n",
        "\n",
        "        # 2. Evaluate models\n",
        "        results_df = self.evaluate_models()\n",
        "\n",
        "        # 3. Plot performance curves\n",
        "        self.plot_roc_curves()\n",
        "\n",
        "        # 4. Feature importance\n",
        "        feature_importance_df = self.feature_importance_analysis()\n",
        "\n",
        "        # 5. Cross-validation\n",
        "        cv_scores = self.cross_validation_scores()\n",
        "\n",
        "        # 6. Save model\n",
        "        self.save_model()\n",
        "\n",
        "        return results_df, feature_importance_df, cv_scores\n",
        "\n",
        "# Execute modeling pipeline\n",
        "predictor = DropoutPredictor(X, y, feature_columns)\n",
        "results_df, feature_importance_df, cv_scores = predictor.full_pipeline()\n",
        "\n",
        "print(\"\\n ANALYSIS COMPLETED SUCCESSFULLY!\")"
      ],
      "metadata": {
        "id": "ntjc2DOVAGxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical significance testing\n",
        "from scipy.stats import ttest_rel\n",
        "import numpy as np\n",
        "\n",
        "# Simulated cross-validation F1 scores\n",
        "lr_scores = [0.788, 0.785, 0.792, 0.790, 0.789]  # Logistic Regression\n",
        "rf_scores = [0.807, 0.809, 0.808, 0.806, 0.810]  # Random Forest\n",
        "xgb_scores = [0.815, 0.817, 0.816, 0.814, 0.818]  # XGBoost\n",
        "\n",
        "# Paired t-tests\n",
        "t_stat_lr_xgb, p_value_lr_xgb = ttest_rel(lr_scores, xgb_scores)\n",
        "t_stat_rf_xgb, p_value_rf_xgb = ttest_rel(rf_scores, xgb_scores)\n",
        "\n",
        "print(f\"XGBoost vs Logistic Regression: p-value = {p_value_lr_xgb:.4f}\")\n",
        "print(f\"XGBoost vs Random Forest: p-value = {p_value_rf_xgb:.4f}\")"
      ],
      "metadata": {
        "id": "R3cFTsgEjhoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Dashboard\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "print(\" Creating Interactive Dashboard...\")\n",
        "\n",
        "class ColabDashboard:\n",
        "    def __init__(self, predictor, processed_df):\n",
        "        self.predictor = predictor\n",
        "        self.df = processed_df\n",
        "        self.create_dashboard()\n",
        "\n",
        "    def create_dashboard(self):\n",
        "        \"\"\"Create an interactive dashboard using Plotly and ipywidgets\"\"\"\n",
        "\n",
        "        print(\" INTERACTIVE STUDENT DROPOUT PREDICTION DASHBOARD\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Display key metrics\n",
        "        self.show_key_metrics()\n",
        "\n",
        "        # Create interactive features\n",
        "        self.create_interactive_plots()\n",
        "\n",
        "        # Create prediction interface\n",
        "        self.create_prediction_interface()\n",
        "\n",
        "    def show_key_metrics(self):\n",
        "        \"\"\"Display key metrics from the analysis\"\"\"\n",
        "        display(Markdown(\"### üìà Key Performance Metrics\"))\n",
        "\n",
        "        best_model_name = self.predictor.best_model_name\n",
        "        best_metrics = self.predictor.metrics[best_model_name]\n",
        "\n",
        "        metrics_text = f\"\"\"\n",
        "**Best Model:** {best_model_name} | **F1-Score:** {best_metrics['F1-Score']:.3f} | **Accuracy:** {best_metrics['Accuracy']:.3f} | **ROC-AUC:** {best_metrics['ROC-AUC']:.3f}\n",
        "\n",
        "üéØ The {best_model_name} model achieved an F1-score of {best_metrics['F1-Score']:.3f} in predicting student dropout risk.\n",
        "\"\"\"\n",
        "        display(Markdown(metrics_text))\n",
        "\n",
        "\n",
        "    def create_interactive_plots(self):\n",
        "        \"\"\"Create interactive plots using Plotly\"\"\"\n",
        "\n",
        "        # 1. Feature Importance Plot\n",
        "        if feature_importance_df is not None:\n",
        "            fig_importance = px.bar(\n",
        "                feature_importance_df.head(10),\n",
        "                x='importance',\n",
        "                y='feature',\n",
        "                orientation='h',\n",
        "                title='<b>Top 10 Most Important Features for Dropout Prediction</b>',\n",
        "                labels={'importance': 'Feature Importance', 'feature': ''},\n",
        "                color='importance',\n",
        "                color_continuous_scale='viridis'\n",
        "            )\n",
        "            fig_importance.update_layout(showlegend=False)\n",
        "            fig_importance.show()\n",
        "\n",
        "        # 2. Target Distribution\n",
        "        risk_counts = self.df['dropout_risk'].value_counts()\n",
        "        fig_pie = px.pie(\n",
        "            values=risk_counts.values,\n",
        "            names=['Not At-Risk', 'At-Risk'],\n",
        "            title='<b>Student Dropout Risk Distribution</b>',\n",
        "            color=['Not At-Risk', 'At-Risk'],\n",
        "            color_discrete_map={'Not At-Risk':'#2ecc71', 'At-Risk':'#e74c3c'}\n",
        "        )\n",
        "        fig_pie.show()\n",
        "\n",
        "        # 3. Correlation Heatmap\n",
        "        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n",
        "        if len(numerical_cols) > 10:\n",
        "            numerical_cols = numerical_cols[:10]  # Limit for readability\n",
        "\n",
        "        corr_matrix = self.df[numerical_cols].corr()\n",
        "\n",
        "        fig_heatmap = px.imshow(\n",
        "            corr_matrix,\n",
        "            title='<b>Feature Correlation Heatmap</b>',\n",
        "            aspect='auto',\n",
        "            color_continuous_scale='RdBu_r'\n",
        "        )\n",
        "        fig_heatmap.show()\n",
        "\n",
        "    def create_prediction_interface(self):\n",
        "        \"\"\"Create an interactive prediction interface\"\"\"\n",
        "\n",
        "        print(\"\\n INTERACTIVE PREDICTION INTERFACE\")\n",
        "        print(\"Adjust the sliders to see how different factors affect dropout risk:\")\n",
        "\n",
        "        # Create interactive sliders for key features\n",
        "        if feature_importance_df is not None:\n",
        "            top_features = feature_importance_df.head(5)['feature'].tolist()\n",
        "        else:\n",
        "            top_features = list(self.df.columns[:5])  # Fallback to first 5 features\n",
        "\n",
        "        # Create sliders dynamically based on feature ranges\n",
        "        sliders = {}\n",
        "        for feature in top_features:\n",
        "            if feature in self.df.columns:\n",
        "                min_val = float(self.df[feature].min())\n",
        "                max_val = float(self.df[feature].max())\n",
        "                mean_val = float(self.df[feature].mean())\n",
        "\n",
        "                sliders[feature] = widgets.FloatSlider(\n",
        "                    value=mean_val,\n",
        "                    min=min_val,\n",
        "                    max=max_val,\n",
        "                    step=(max_val - min_val) / 100,\n",
        "                    description=feature[:20] + ':' if len(feature) > 20 else feature + ':',\n",
        "                    continuous_update=False\n",
        "                )\n",
        "\n",
        "        # Create interactive function\n",
        "        def update_prediction(**kwargs):\n",
        "            input_features = {}\n",
        "            for feature, value in kwargs.items():\n",
        "                input_features[feature] = value\n",
        "\n",
        "            # Create input array\n",
        "            input_array = np.zeros((1, len(self.predictor.feature_names)))\n",
        "            for i, feat in enumerate(self.predictor.feature_names):\n",
        "                if feat in input_features:\n",
        "                    input_array[0, i] = input_features[feat]\n",
        "                else:\n",
        "                    # Use mean value for missing features\n",
        "                    input_array[0, i] = self.df[feat].mean()\n",
        "\n",
        "            # Scale and predict\n",
        "            input_scaled = self.predictor.scaler.transform(input_array)\n",
        "            probability = self.predictor.best_model.predict_proba(input_scaled)[0, 1]\n",
        "\n",
        "            print(f\"\\n PREDICTION RESULTS:\")\n",
        "            print(f\" Dropout Risk Probability: {probability:.1%}\")\n",
        "            print(f\" Risk Level: {'HIGH' if probability > 0.5 else 'LOW'}\")\n",
        "\n",
        "            if probability > 0.7:\n",
        "                print(\"üö® URGENT: High dropout risk detected! Immediate intervention recommended.\")\n",
        "            elif probability > 0.5:\n",
        "                print(\"‚ö†Ô∏è  WARNING: Moderate dropout risk. Monitor closely and provide support.\")\n",
        "            else:\n",
        "                print(\"‚úÖ GOOD: Low dropout risk. Continue regular monitoring.\")\n",
        "\n",
        "            # Show progress bar\n",
        "            progress_html = f\"\"\"\n",
        "            <div style=\"background: #f0f0f0; border-radius: 10px; padding: 10px; margin: 10px 0;\">\n",
        "                <div style=\"background: linear-gradient(90deg, #2ecc71 {probability*100}%, #e74c3c {probability*100}%);\n",
        "                          height: 20px; border-radius: 5px; width: 100%;\"></div>\n",
        "                <div style=\"text-align: center; margin-top: 5px; font-weight: bold;\">\n",
        "                    Risk Score: {probability:.1%}\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(progress_html))\n",
        "\n",
        "        # Create interactive widget\n",
        "        print(\"\\nüîß Adjust the sliders and watch the prediction update:\")\n",
        "        interact(update_prediction, **sliders)\n",
        "\n",
        "# Create the dashboard\n",
        "dashboard = ColabDashboard(predictor, processed_df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n Generated Outputs:\")\n",
        "print(\" Trained machine learning models\")\n",
        "print(\" Model performance metrics\")\n",
        "print(\" Feature importance analysis\")\n",
        "print(\"Interactive visualizations\")\n",
        "print(\"Prediction interface\")\n",
        "print(\"Saved model: 'student_dropout_model.pkl'\")\n",
        "\n",
        "print(\"\\n Next Steps:\")\n",
        "print(\"1. The model is ready for predicting student dropout risk\")\n",
        "print(\"2. Use the interactive interface to test different student profiles\")\n"
      ],
      "metadata": {
        "id": "4WU-21M-_rLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the trained model and create report\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "print(\" Preparing files for download...\")\n",
        "\n",
        "# Save comprehensive results\n",
        "results = {\n",
        "    'dataset_info': {\n",
        "        'name': 'Predict Students Dropout and Academic Success',\n",
        "        'source': 'UCI Machine Learning Repository',\n",
        "        'id': 697,\n",
        "        'samples': len(df),\n",
        "        'features': len(feature_columns),\n",
        "        'dropout_rate': df['dropout_risk'].mean()\n",
        "    },\n",
        "    'best_model': {\n",
        "        'name': predictor.best_model_name,\n",
        "        'metrics': predictor.metrics[predictor.best_model_name],\n",
        "        'feature_importance': feature_importance_df.head(10).to_dict() if feature_importance_df is not None else None\n",
        "    },\n",
        "    'all_models': predictor.metrics,\n",
        "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "import json\n",
        "with open('analysis_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Create a summary report\n",
        "report_content = f\"\"\"\n",
        "STUDENT DROPOUT PREDICTION ANALYSIS REPORT\n",
        "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "\n",
        "DATASET INFORMATION:\n",
        "- Source: UCI Machine Learning Repository (ID: 697)\n",
        "- Total Students: {len(df):,}\n",
        "- Number of Features: {len(feature_columns)}\n",
        "- Dropout Rate: {df['dropout_risk'].mean():.2%}\n",
        "\n",
        "MODEL PERFORMANCE:\n",
        "- Best Model: {predictor.best_model_name}\n",
        "- F1-Score: {predictor.metrics[predictor.best_model_name]['F1-Score']:.3f}\n",
        "- Accuracy: {predictor.metrics[predictor.best_model_name]['Accuracy']:.3f}\n",
        "- ROC-AUC: {predictor.metrics[predictor.best_model_name]['ROC-AUC']:.3f}\n",
        "\n",
        "TOP PREDICTIVE FEATURES:\n",
        "\"\"\"\n",
        "\n",
        "if feature_importance_df is not None:\n",
        "    for i, row in feature_importance_df.head(5).iterrows():\n",
        "        report_content += f\"{i+1}. {row['feature']}: {row['importance']:.3f}\\n\"\n",
        "\n",
        "with open('analysis_report.txt', 'w') as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "print(\"Generated Files:\")\n",
        "print(\"   - student_dropout_model.pkl (Trained model)\")\n",
        "print(\"   - analysis_results.json (Detailed results)\")\n",
        "print(\"   - analysis_report.txt (Summary report)\")\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"\\n Downloading files...\")\n",
        "files.download('student_dropout_model.pkl')\n",
        "files.download('analysis_results.json')\n",
        "files.download('analysis_report.txt')\n",
        "\n",
        "print(\"\\n ALL DONE! Your complete student dropout analysis is ready!\")"
      ],
      "metadata": {
        "id": "pmxG1neaAlPn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}